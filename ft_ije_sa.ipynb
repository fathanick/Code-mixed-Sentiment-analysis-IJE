{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d8d35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 10:20:17.132636: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-15 10:20:18.017643: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-15 10:20:18.017759: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-15 10:20:18.017770: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2351ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  1350\n",
      "Val:  116\n",
      "Test:  463\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('../dataset/ije_sa/ije_sa.xlsx')\n",
    "train_data = pd.read_excel('../dataset/ije_sa/train_set.xlsx')\n",
    "val_data = pd.read_excel('../dataset/ije_sa/validation_set.xlsx')\n",
    "test_data = pd.read_excel('../dataset/ije_sa/test_set.xlsx')\n",
    "\n",
    "# Length train, val, and test\n",
    "print(\"Train: \",len(train_data))\n",
    "print(\"Val: \",len(val_data))\n",
    "print(\"Test: \",len(test_data))\n",
    "\n",
    "tags = np.unique(df['label'])\n",
    "num_labels = len(tags)\n",
    "max_length = 128\n",
    "label2id = {t: i for i, t in enumerate(tags)}\n",
    "id2label = {i: t for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b3529d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(model_name):\n",
    "    global tokenizer\n",
    "    global data_collator\n",
    "    global tr_model\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=128)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc910494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # process the input sequence\n",
    "    tokenized_input = tokenizer(examples[\"tweet\"], \n",
    "                                truncation=True, \n",
    "                                padding='max_length', \n",
    "                                max_length=max_length)\n",
    "    # process the labels\n",
    "    tokenized_input['label'] = [label2id[lb] for lb in examples['label']]\n",
    "    \n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2d26d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    X_train = Dataset.from_pandas(train_data)\n",
    "    X_val = Dataset.from_pandas(val_data)\n",
    "    X_test = Dataset.from_pandas(test_data)\n",
    "    \n",
    "    tokenized_train_data = X_train.map(tokenize_function, batched=True)\n",
    "    tokenized_val_data = X_val.map(tokenize_function, batched=True)\n",
    "    tokenized_test_data = X_test.map(tokenize_function, batched=True)\n",
    "    \n",
    "    return tokenized_train_data, tokenized_val_data, tokenized_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c3049cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    \n",
    "    true_labels = [tags[l] for l in labels] \n",
    "    true_predictions = [tags[pr] for pr in pred]\n",
    "\n",
    "    report = classification_report(true_labels, true_predictions, digits=4)\n",
    "    acc = accuracy_score(y_true=true_labels, y_pred=true_predictions)\n",
    "    rec = recall_score(y_true=true_labels, y_pred=true_predictions, average='macro')\n",
    "    prec = precision_score(y_true=true_labels, y_pred=true_predictions, average='macro')\n",
    "    f1 = f1_score(y_true=true_labels, y_pred=true_predictions, average='macro', zero_division=1.0)\n",
    "\n",
    "    print(\"Classification Report:\\n{}\".format(report))\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b4ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, output_dir, \n",
    "                learning_rate, train_batch_size, \n",
    "                eval_batch_size, num_epochs, weight_decay):\n",
    "    model, tokenizer = model_init(model_name)\n",
    "    train_tokenized, val_tokenized, test_tokenized = preprocessing()\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        logging_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit = 1,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        per_device_eval_batch_size=eval_batch_size,\n",
    "        weight_decay=weight_decay,\n",
    "        #push_to_hub=True,\n",
    "        metric_for_best_model = \"f1\",\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=val_tokenized,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    #trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c33df428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(model_name):\n",
    "    model, tokenizer = model_init(model_name)\n",
    "    train_tokenized, val_tokenized, test_tokenized = preprocessing()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # trainer.predict(test_tokenized)\n",
    "    # trainer.push_to_hub(commit_message=\"Test complete\")\n",
    "    print(trainer.predict(test_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceecd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_name, output_dir, best_params):\n",
    "    start = time.time()\n",
    "     # load json file containing best params\n",
    "    best_params = best_params\n",
    "\n",
    "    with open(best_params, 'r') as js:\n",
    "        data = json.load(js)\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    # define best params\n",
    "    learning_rate = data['learning_rate']\n",
    "    train_batch_size = data['per_device_train_batch_size']\n",
    "    eval_batch_size = data['per_device_eval_batch_size']\n",
    "    weight_decay = data['weight_decay']\n",
    "    num_epochs = data['num_epochs']\n",
    "\n",
    "    # training\n",
    "    train_model(model_name=model_name, \n",
    "                output_dir=output_dir,\n",
    "                learning_rate=learning_rate,\n",
    "                train_batch_size=train_batch_size,\n",
    "                eval_batch_size=eval_batch_size,\n",
    "                num_epochs=num_epochs,\n",
    "                weight_decay=weight_decay)\n",
    "\n",
    "    print('Training finished!')\n",
    "    \n",
    "    # prediction\n",
    "    predict_model(model_name=f'{output_dir}')\n",
    "    print('Prediction finished!')\n",
    "\n",
    "    \n",
    "    end = time.time()\n",
    "    exec_time = (end - start) / 60\n",
    "    print(f'Total time: {exec_time} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a16962a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 8, 'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 8, 'weight_decay': 0.0007450441482348415}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at fathan/indojave-codemixed-bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at fathan/indojave-codemixed-bert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520adb79d8d24e419f82577162b24f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5334e4113a524f10942b65b1743faafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e924d05ca9435bb48b237f5b7241f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1350\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 176\n",
      "  Number of trainable parameters = 124443651\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfathanel\u001b[0m (\u001b[33mfathanick_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter-145230403/00-PhD Project/CM-FINE-TUNING/IJE_SA/wandb/run-20231015_102030-waadr7dn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fathanick_lab/huggingface/runs/waadr7dn' target=\"_blank\">balmy-darkness-12</a></strong> to <a href='https://wandb.ai/fathanick_lab/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fathanick_lab/huggingface' target=\"_blank\">https://wandb.ai/fathanick_lab/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fathanick_lab/huggingface/runs/waadr7dn' target=\"_blank\">https://wandb.ai/fathanick_lab/huggingface/runs/waadr7dn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 02:03, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.482800</td>\n",
       "      <td>0.287499</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>0.913801</td>\n",
       "      <td>0.903704</td>\n",
       "      <td>0.905385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.089800</td>\n",
       "      <td>0.381423</td>\n",
       "      <td>0.922414</td>\n",
       "      <td>0.934929</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>0.921286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.249006</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.953574</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.949039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.329842</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.934342</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.931743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.378404</td>\n",
       "      <td>0.922414</td>\n",
       "      <td>0.930747</td>\n",
       "      <td>0.921296</td>\n",
       "      <td>0.922386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.381763</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.943915</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.939374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.355226</td>\n",
       "      <td>0.922414</td>\n",
       "      <td>0.924649</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.922772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.351475</td>\n",
       "      <td>0.922414</td>\n",
       "      <td>0.924649</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.922772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9722    0.8750    0.9211        40\n",
      "     Neutral     0.9394    0.8611    0.8986        36\n",
      "    Positive     0.8298    0.9750    0.8966        40\n",
      "\n",
      "    accuracy                         0.9052       116\n",
      "   macro avg     0.9138    0.9037    0.9054       116\n",
      "weighted avg     0.9129    0.9052    0.9056       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-22\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-22/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-22/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-22/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-22/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-7] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9750    0.9750    0.9750        40\n",
      "     Neutral     1.0000    0.8056    0.8923        36\n",
      "    Positive     0.8298    0.9750    0.8966        40\n",
      "\n",
      "    accuracy                         0.9224       116\n",
      "   macro avg     0.9349    0.9185    0.9213       116\n",
      "weighted avg     0.9327    0.9224    0.9223       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-44\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-44/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-44/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-44/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-44/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-22] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9744    0.9500    0.9620        40\n",
      "     Neutral     1.0000    0.9167    0.9565        36\n",
      "    Positive     0.8864    0.9750    0.9286        40\n",
      "\n",
      "    accuracy                         0.9483       116\n",
      "   macro avg     0.9536    0.9472    0.9490       116\n",
      "weighted avg     0.9520    0.9483    0.9488       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-66\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-66/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-66/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-66/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-66/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-44] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9487    0.9250    0.9367        40\n",
      "     Neutral     0.9706    0.9167    0.9429        36\n",
      "    Positive     0.8837    0.9500    0.9157        40\n",
      "\n",
      "    accuracy                         0.9310       116\n",
      "   macro avg     0.9343    0.9306    0.9317       116\n",
      "weighted avg     0.9331    0.9310    0.9314       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-88\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-88/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-88/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-88/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-88/special_tokens_map.json\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     1.0000    0.8750    0.9333        40\n",
      "     Neutral     0.9412    0.8889    0.9143        36\n",
      "    Positive     0.8511    1.0000    0.9195        40\n",
      "\n",
      "    accuracy                         0.9224       116\n",
      "   macro avg     0.9307    0.9213    0.9224       116\n",
      "weighted avg     0.9304    0.9224    0.9227       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-110\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-110/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-110/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-110/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-110/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-88] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     1.0000    0.9000    0.9474        40\n",
      "     Neutral     0.9429    0.9167    0.9296        36\n",
      "    Positive     0.8889    1.0000    0.9412        40\n",
      "\n",
      "    accuracy                         0.9397       116\n",
      "   macro avg     0.9439    0.9389    0.9394       116\n",
      "weighted avg     0.9440    0.9397    0.9397       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-132\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-132/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-132/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-132/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-132/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-110] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9474    0.9000    0.9231        40\n",
      "     Neutral     0.9429    0.9167    0.9296        36\n",
      "    Positive     0.8837    0.9500    0.9157        40\n",
      "\n",
      "    accuracy                         0.9224       116\n",
      "   macro avg     0.9246    0.9222    0.9228       116\n",
      "weighted avg     0.9240    0.9224    0.9225       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-154\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-154/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-154/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-154/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-154/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-132] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9474    0.9000    0.9231        40\n",
      "     Neutral     0.9429    0.9167    0.9296        36\n",
      "    Positive     0.8837    0.9500    0.9157        40\n",
      "\n",
      "    accuracy                         0.9224       116\n",
      "   macro avg     0.9246    0.9222    0.9228       116\n",
      "weighted avg     0.9240    0.9224    0.9225       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-176\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-176/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-154] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-66 (score: 0.9490394947191865).\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/checkpoint-176] due to args.save_total_limit\n",
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/special_tokens_map.json\n",
      "loading configuration file IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert\",\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {},\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4af5c527dd4c269700b4d6b9908cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a35d9a7523d441ddaaef2872e4d9be8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfbe2e12e644d1e9ea98aef05bfe42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 463\n",
      "  Batch size = 32\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8961    0.9650    0.9293       143\n",
      "     Neutral     0.9869    0.9264    0.9557       163\n",
      "    Positive     0.9423    0.9363    0.9393       157\n",
      "\n",
      "    accuracy                         0.9417       463\n",
      "   macro avg     0.9418    0.9426    0.9414       463\n",
      "weighted avg     0.9437    0.9417    0.9420       463\n",
      "\n",
      "PredictionOutput(predictions=array([[ 4.514627  , -3.9299123 , -1.1667176 ],\n",
      "       [ 4.871687  , -3.0491009 , -2.2938764 ],\n",
      "       [ 4.8515363 , -3.373619  , -1.9560899 ],\n",
      "       ...,\n",
      "       [-2.054923  ,  2.6831512 ,  0.21166822],\n",
      "       [ 4.772097  , -3.1108046 , -2.0322428 ],\n",
      "       [-3.312058  , -0.2243209 ,  3.7559423 ]], dtype=float32), label_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
      "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0,\n",
      "       0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0,\n",
      "       2]), metrics={'test_loss': 0.2579711675643921, 'test_accuracy': 0.9416846652267818, 'test_precision': 0.9417798976622507, 'test_recall': 0.942573688539067, 'test_f1': 0.9414287521417379, 'test_runtime': 2.6129, 'test_samples_per_second': 177.199, 'test_steps_per_second': 5.741})\n",
      "Prediction finished!\n",
      "Total time: 2.7927383462587994 minutes\n"
     ]
    }
   ],
   "source": [
    "#1 fathan/indojave-codemixed-bert-base\n",
    "main(\n",
    "    model_name = 'fathan/indojave-codemixed-bert-base',\n",
    "    output_dir = 'IJE_SA_MODEL/ije-sa-ft-indojave-cm-bert',\n",
    "    best_params = 'best_parameters/indojave-cm-bert.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb65a69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 6, 'learning_rate': 5e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'weight_decay': 0.0006240436334981423}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-roberta-base/snapshots/487f71ebf847f059c180212fc636a7efbb0cbf70/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"fathan/indojave-codemixed-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-roberta-base/snapshots/487f71ebf847f059c180212fc636a7efbb0cbf70/pytorch_model.bin\n",
      "Some weights of the model checkpoint at fathan/indojave-codemixed-roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at fathan/indojave-codemixed-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file vocab.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-roberta-base/snapshots/487f71ebf847f059c180212fc636a7efbb0cbf70/vocab.json\n",
      "loading file merges.txt from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-roberta-base/snapshots/487f71ebf847f059c180212fc636a7efbb0cbf70/merges.txt\n",
      "loading file tokenizer.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-roberta-base/snapshots/487f71ebf847f059c180212fc636a7efbb0cbf70/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-roberta-base/snapshots/487f71ebf847f059c180212fc636a7efbb0cbf70/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-roberta-base/snapshots/487f71ebf847f059c180212fc636a7efbb0cbf70/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358421c1ba724758aeb15af7e1fbd5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582f78aa23744c2e9d364309ef3dad35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97c457bccaa468a84c0060b945ea389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1350\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 258\n",
      "  Number of trainable parameters = 125979651\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='258' max='258' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [258/258 02:09, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.487400</td>\n",
       "      <td>0.352361</td>\n",
       "      <td>0.887931</td>\n",
       "      <td>0.901984</td>\n",
       "      <td>0.887963</td>\n",
       "      <td>0.889718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.353335</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.940254</td>\n",
       "      <td>0.929630</td>\n",
       "      <td>0.931482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.333951</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.943074</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.939729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.286641</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.941173</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.939351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.313503</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.943074</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.939729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.355743</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.936508</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.931531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     1.0000    0.8250    0.9041        40\n",
      "     Neutral     0.9143    0.8889    0.9014        36\n",
      "    Positive     0.7917    0.9500    0.8636        40\n",
      "\n",
      "    accuracy                         0.8879       116\n",
      "   macro avg     0.9020    0.8880    0.8897       116\n",
      "weighted avg     0.9016    0.8879    0.8893       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-43\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-43/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-43/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-43/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-43/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-24] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     1.0000    0.9000    0.9474        40\n",
      "     Neutral     0.9697    0.8889    0.9275        36\n",
      "    Positive     0.8511    1.0000    0.9195        40\n",
      "\n",
      "    accuracy                         0.9310       116\n",
      "   macro avg     0.9403    0.9296    0.9315       116\n",
      "weighted avg     0.9392    0.9310    0.9316       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-86\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-86/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-86/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-86/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-86/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-43] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     1.0000    0.9250    0.9610        40\n",
      "     Neutral     0.9429    0.9167    0.9296        36\n",
      "    Positive     0.8864    0.9750    0.9286        40\n",
      "\n",
      "    accuracy                         0.9397       116\n",
      "   macro avg     0.9431    0.9389    0.9397       116\n",
      "weighted avg     0.9431    0.9397    0.9401       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-129\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-129/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-129/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-129/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-129/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-86] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9737    0.9250    0.9487        40\n",
      "     Neutral     0.9429    0.9167    0.9296        36\n",
      "    Positive     0.9070    0.9750    0.9398        40\n",
      "\n",
      "    accuracy                         0.9397       116\n",
      "   macro avg     0.9412    0.9389    0.9394       116\n",
      "weighted avg     0.9411    0.9397    0.9397       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-172\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-172/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-172/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-172/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-172/special_tokens_map.json\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     1.0000    0.9250    0.9610        40\n",
      "     Neutral     0.9429    0.9167    0.9296        36\n",
      "    Positive     0.8864    0.9750    0.9286        40\n",
      "\n",
      "    accuracy                         0.9397       116\n",
      "   macro avg     0.9431    0.9389    0.9397       116\n",
      "weighted avg     0.9431    0.9397    0.9401       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-215\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-215/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-215/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-215/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-215/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-172] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     1.0000    0.9000    0.9474        40\n",
      "     Neutral     0.9429    0.9167    0.9296        36\n",
      "    Positive     0.8667    0.9750    0.9176        40\n",
      "\n",
      "    accuracy                         0.9310       116\n",
      "   macro avg     0.9365    0.9306    0.9315       116\n",
      "weighted avg     0.9363    0.9310    0.9316       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-258\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-258/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-258/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-258/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-258/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-215] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-129 (score: 0.9397292847997072).\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/checkpoint-258] due to args.save_total_limit\n",
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/special_tokens_map.json\n",
      "loading configuration file IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta\",\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {},\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading weights file IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f754f1f902764cf69ebc4ce9405e14da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3763529d8c542568d71c8e132c6cd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9b49bb9b874374bbd6e5d5b77af0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 463\n",
      "  Batch size = 32\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8792    0.9161    0.8973       143\n",
      "     Neutral     0.9281    0.9509    0.9394       163\n",
      "    Positive     0.9252    0.8662    0.8947       157\n",
      "\n",
      "    accuracy                         0.9114       463\n",
      "   macro avg     0.9108    0.9111    0.9105       463\n",
      "weighted avg     0.9120    0.9114    0.9112       463\n",
      "\n",
      "PredictionOutput(predictions=array([[ 4.9040475 , -2.8733916 , -2.01359   ],\n",
      "       [ 4.9321246 , -2.8725445 , -2.077031  ],\n",
      "       [ 4.9500737 , -2.7731822 , -2.2550828 ],\n",
      "       ...,\n",
      "       [-2.851556  ,  4.298276  , -1.5379208 ],\n",
      "       [ 4.6373005 , -2.0078518 , -2.6047714 ],\n",
      "       [-3.2134812 , -0.44903287,  4.296734  ]], dtype=float32), label_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
      "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0,\n",
      "       0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0,\n",
      "       2]), metrics={'test_loss': 0.4192201793193817, 'test_accuracy': 0.9114470842332614, 'test_precision': 0.9108361371581815, 'test_recall': 0.9110820665664164, 'test_f1': 0.9104636851572684, 'test_runtime': 2.0874, 'test_samples_per_second': 221.811, 'test_steps_per_second': 7.186})\n",
      "Prediction finished!\n",
      "Total time: 2.340323575337728 minutes\n"
     ]
    }
   ],
   "source": [
    "#2 fathan/indojave-codemixed-roberta-base\n",
    "\n",
    "main(\n",
    "    model_name = 'fathan/indojave-codemixed-roberta-base',\n",
    "    output_dir = 'IJE_SA_MODEL/ije-sa-ft-indojave-cm-roberta',\n",
    "    best_params = 'best_parameters/indojave-cm-roberta.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0093522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 5, 'learning_rate': 5e-05, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'weight_decay': 0.00010054838958197635}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-indobert-base/snapshots/4a4c322f69fa8ae518250f539dc4b25ba2448885/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"fathan/indojave-codemixed-indobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-indobert-base/snapshots/4a4c322f69fa8ae518250f539dc4b25ba2448885/pytorch_model.bin\n",
      "Some weights of the model checkpoint at fathan/indojave-codemixed-indobert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at fathan/indojave-codemixed-indobert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file vocab.txt from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-indobert-base/snapshots/4a4c322f69fa8ae518250f539dc4b25ba2448885/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-indobert-base/snapshots/4a4c322f69fa8ae518250f539dc4b25ba2448885/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-indobert-base/snapshots/4a4c322f69fa8ae518250f539dc4b25ba2448885/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-indobert-base/snapshots/4a4c322f69fa8ae518250f539dc4b25ba2448885/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193bbf80ce9f48638ef43010cbe4180f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8389ab246890404d899f16b06909a02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0317228e5604c95b1388c436d6027c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1350\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 215\n",
      "  Number of trainable parameters = 110560515\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='215' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [215/215 01:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>0.174087</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.949419</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.948673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.079700</td>\n",
       "      <td>0.207235</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.953159</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.948358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.169961</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.949720</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.948026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.194439</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>0.960659</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.957094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.203072</td>\n",
       "      <td>0.956897</td>\n",
       "      <td>0.959893</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.956532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9500    0.9500    0.9500        40\n",
      "     Neutral     0.9714    0.9444    0.9577        36\n",
      "    Positive     0.9268    0.9500    0.9383        40\n",
      "\n",
      "    accuracy                         0.9483       116\n",
      "   macro avg     0.9494    0.9481    0.9487       116\n",
      "weighted avg     0.9487    0.9483    0.9484       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-43\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-43/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-43/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-43/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-43/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-8] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     1.0000    0.9250    0.9610        40\n",
      "     Neutral     0.9706    0.9167    0.9429        36\n",
      "    Positive     0.8889    1.0000    0.9412        40\n",
      "\n",
      "    accuracy                         0.9483       116\n",
      "   macro avg     0.9532    0.9472    0.9484       116\n",
      "weighted avg     0.9526    0.9483    0.9485       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-86\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-86/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-86/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-86/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-86/special_tokens_map.json\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9500    0.9500    0.9500        40\n",
      "     Neutral     0.9706    0.9167    0.9429        36\n",
      "    Positive     0.9286    0.9750    0.9512        40\n",
      "\n",
      "    accuracy                         0.9483       116\n",
      "   macro avg     0.9497    0.9472    0.9480       116\n",
      "weighted avg     0.9490    0.9483    0.9482       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-129\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-129/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-129/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-129/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-129/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-86] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9750    0.9750    0.9750        40\n",
      "     Neutral     1.0000    0.9167    0.9565        36\n",
      "    Positive     0.9070    0.9750    0.9398        40\n",
      "\n",
      "    accuracy                         0.9569       116\n",
      "   macro avg     0.9607    0.9556    0.9571       116\n",
      "weighted avg     0.9593    0.9569    0.9571       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-172\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-172/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-172/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-172/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-172/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-43] due to args.save_total_limit\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-129] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     1.0000    0.9500    0.9744        40\n",
      "     Neutral     0.9706    0.9167    0.9429        36\n",
      "    Positive     0.9091    1.0000    0.9524        40\n",
      "\n",
      "    accuracy                         0.9569       116\n",
      "   macro avg     0.9599    0.9556    0.9565       116\n",
      "weighted avg     0.9595    0.9569    0.9570       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-215\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-215/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-215/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-215/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-215/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-172 (score: 0.9570935917583377).\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/checkpoint-215] due to args.save_total_limit\n",
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/special_tokens_map.json\n",
      "loading configuration file IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert\",\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {},\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438bbbb4b04e429fb2b1f86cb866f04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1d002938d0467380af67e7ddb7db2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1a17fa9f184a91bce7a28350d9007b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 463\n",
      "  Batch size = 32\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9211    0.9790    0.9492       143\n",
      "     Neutral     0.9936    0.9571    0.9750       163\n",
      "    Positive     0.9675    0.9490    0.9582       157\n",
      "\n",
      "    accuracy                         0.9611       463\n",
      "   macro avg     0.9607    0.9617    0.9608       463\n",
      "weighted avg     0.9624    0.9611    0.9613       463\n",
      "\n",
      "PredictionOutput(predictions=array([[ 4.9390697, -3.7292545, -1.4951298],\n",
      "       [ 5.1308875, -3.380357 , -2.1732352],\n",
      "       [ 5.0868425, -3.2506335, -2.1547565],\n",
      "       ...,\n",
      "       [-2.2854433, -1.2984508,  4.1714864],\n",
      "       [ 4.988334 , -3.5435681, -1.9317676],\n",
      "       [-2.1331456, -2.3069801,  5.1612396]], dtype=float32), label_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
      "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0,\n",
      "       0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0,\n",
      "       2]), metrics={'test_loss': 0.16494408249855042, 'test_accuracy': 0.9611231101511879, 'test_precision': 0.9607385574532742, 'test_recall': 0.9617069265773889, 'test_f1': 0.960783966428688, 'test_runtime': 2.5545, 'test_samples_per_second': 181.251, 'test_steps_per_second': 5.872})\n",
      "Prediction finished!\n",
      "Total time: 1.8511721928914389 minutes\n"
     ]
    }
   ],
   "source": [
    "#3 fathan/indojave-codemixed-indobert-base\n",
    "main(\n",
    "    model_name = 'fathan/indojave-codemixed-indobert-base',\n",
    "    output_dir = 'IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobert',\n",
    "    best_params = 'best_parameters/indojave-cm-indobert.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "504986d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 5, 'learning_rate': 0.0003, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 32, 'weight_decay': 0.003234558453531053}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-indobertweet-base/snapshots/513d49f87b1bf3519a35fc5ed104e6a39ab97fae/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"fathan/indojave-codemixed-indobertweet-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-indobertweet-base/snapshots/513d49f87b1bf3519a35fc5ed104e6a39ab97fae/pytorch_model.bin\n",
      "Some weights of the model checkpoint at fathan/indojave-codemixed-indobertweet-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at fathan/indojave-codemixed-indobertweet-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file vocab.txt from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-indobertweet-base/snapshots/513d49f87b1bf3519a35fc5ed104e6a39ab97fae/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-indobertweet-base/snapshots/513d49f87b1bf3519a35fc5ed104e6a39ab97fae/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-indobertweet-base/snapshots/513d49f87b1bf3519a35fc5ed104e6a39ab97fae/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--fathan--indojave-codemixed-indobertweet-base/snapshots/513d49f87b1bf3519a35fc5ed104e6a39ab97fae/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be53742efc94842bbf88b003d57be2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cedc98782bb410780acecb6ace74bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7676aef864448119c00d6c7ffc5a24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1350\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 110\n",
      "  Number of trainable parameters = 110560515\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 01:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.486200</td>\n",
       "      <td>0.366113</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>0.908125</td>\n",
       "      <td>0.905556</td>\n",
       "      <td>0.906193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.278442</td>\n",
       "      <td>0.913793</td>\n",
       "      <td>0.929516</td>\n",
       "      <td>0.912037</td>\n",
       "      <td>0.915101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.140834</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.950697</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.948741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>0.256124</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.943915</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.939374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.209986</td>\n",
       "      <td>0.948276</td>\n",
       "      <td>0.950641</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.948214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9211    0.8750    0.8974        40\n",
      "     Neutral     0.9429    0.9167    0.9296        36\n",
      "    Positive     0.8605    0.9250    0.8916        40\n",
      "\n",
      "    accuracy                         0.9052       116\n",
      "   macro avg     0.9081    0.9056    0.9062       116\n",
      "weighted avg     0.9069    0.9052    0.9054       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-22\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-22/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-22/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-22/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-22/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-6] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9722    0.8750    0.9211        40\n",
      "     Neutral     1.0000    0.8611    0.9254        36\n",
      "    Positive     0.8163    1.0000    0.8989        40\n",
      "\n",
      "    accuracy                         0.9138       116\n",
      "   macro avg     0.9295    0.9120    0.9151       116\n",
      "weighted avg     0.9271    0.9138    0.9147       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-44\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-44/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-44/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-44/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-44/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-22] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9737    0.9250    0.9487        40\n",
      "     Neutral     0.9714    0.9444    0.9577        36\n",
      "    Positive     0.9070    0.9750    0.9398        40\n",
      "\n",
      "    accuracy                         0.9483       116\n",
      "   macro avg     0.9507    0.9481    0.9487       116\n",
      "weighted avg     0.9500    0.9483    0.9484       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-66\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-66/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-66/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-66/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-66/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-44] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     1.0000    0.9000    0.9474        40\n",
      "     Neutral     0.9429    0.9167    0.9296        36\n",
      "    Positive     0.8889    1.0000    0.9412        40\n",
      "\n",
      "    accuracy                         0.9397       116\n",
      "   macro avg     0.9439    0.9389    0.9394       116\n",
      "weighted avg     0.9440    0.9397    0.9397       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-88\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-88/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-88/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-88/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-88/special_tokens_map.json\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9744    0.9500    0.9620        40\n",
      "     Neutral     0.9706    0.9167    0.9429        36\n",
      "    Positive     0.9070    0.9750    0.9398        40\n",
      "\n",
      "    accuracy                         0.9483       116\n",
      "   macro avg     0.9506    0.9472    0.9482       116\n",
      "weighted avg     0.9500    0.9483    0.9484       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-110\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-110/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-110/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-110/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-110/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-88] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-66 (score: 0.948741154578589).\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/checkpoint-110] due to args.save_total_limit\n",
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/special_tokens_map.json\n",
      "loading configuration file IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet\",\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {},\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64dad98fc6a493b88246bb9858ca49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be83cf96673d417da861492bdc141b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb3366023b444eda7034949c63cc406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 463\n",
      "  Batch size = 32\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9205    0.9720    0.9456       143\n",
      "     Neutral     0.9873    0.9571    0.9720       163\n",
      "    Positive     0.9416    0.9236    0.9325       157\n",
      "\n",
      "    accuracy                         0.9503       463\n",
      "   macro avg     0.9498    0.9509    0.9500       463\n",
      "weighted avg     0.9512    0.9503    0.9504       463\n",
      "\n",
      "PredictionOutput(predictions=array([[ 4.420567 , -3.4320877, -1.1812421],\n",
      "       [ 4.558454 , -2.7492235, -2.0057154],\n",
      "       [ 4.575959 , -2.9644396, -1.8043576],\n",
      "       ...,\n",
      "       [ 0.573043 , -3.0729578,  3.0017962],\n",
      "       [ 4.56535  , -2.8024833, -1.8594264],\n",
      "       [-0.7342177, -2.7236228,  4.0794168]], dtype=float32), label_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
      "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0,\n",
      "       0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0,\n",
      "       2]), metrics={'test_loss': 0.24008430540561676, 'test_accuracy': 0.9503239740820735, 'test_precision': 0.9498100050116145, 'test_recall': 0.9508833552442634, 'test_f1': 0.9500055774531067, 'test_runtime': 2.4013, 'test_samples_per_second': 192.815, 'test_steps_per_second': 6.247})\n",
      "Prediction finished!\n",
      "Total time: 1.3922390699386598 minutes\n"
     ]
    }
   ],
   "source": [
    "#4 fathan/indojave-codemixed-indobertweet-base\n",
    "main(\n",
    "    model_name = 'fathan/indojave-codemixed-indobertweet-base',\n",
    "    output_dir = 'IJE_SA_MODEL/ije-sa-ft-indojave-cm-indobertweet',\n",
    "    best_params = 'best_parameters/indojave-cm-indobertweet.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "074407cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 9, 'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 64, 'weight_decay': 0.0001287688045739695}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/7ccb3cd0f5b08ffbaa465aade22328e8600e23eb/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"indolem/indobert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/7ccb3cd0f5b08ffbaa465aade22328e8600e23eb/pytorch_model.bin\n",
      "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/7ccb3cd0f5b08ffbaa465aade22328e8600e23eb/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"indolem/indobert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/7ccb3cd0f5b08ffbaa465aade22328e8600e23eb/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/7ccb3cd0f5b08ffbaa465aade22328e8600e23eb/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/7ccb3cd0f5b08ffbaa465aade22328e8600e23eb/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/7ccb3cd0f5b08ffbaa465aade22328e8600e23eb/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/7ccb3cd0f5b08ffbaa465aade22328e8600e23eb/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"indolem/indobert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobert-base-uncased/snapshots/7ccb3cd0f5b08ffbaa465aade22328e8600e23eb/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"indolem/indobert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3935fe89ad2b4c2c8102f2e97f47065a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ee5202bcf8462bae0083218c7d13e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f52236c68f42cdbaf63a976372e12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1350\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 198\n",
      "  Number of trainable parameters = 110560515\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 02:03, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.938300</td>\n",
       "      <td>0.773354</td>\n",
       "      <td>0.629310</td>\n",
       "      <td>0.749769</td>\n",
       "      <td>0.630556</td>\n",
       "      <td>0.619037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.661000</td>\n",
       "      <td>0.384270</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.913580</td>\n",
       "      <td>0.875926</td>\n",
       "      <td>0.881661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>0.379866</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.867368</td>\n",
       "      <td>0.842593</td>\n",
       "      <td>0.846652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>0.334010</td>\n",
       "      <td>0.887931</td>\n",
       "      <td>0.900282</td>\n",
       "      <td>0.885185</td>\n",
       "      <td>0.888180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>0.279068</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>0.920257</td>\n",
       "      <td>0.903704</td>\n",
       "      <td>0.906329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.250180</td>\n",
       "      <td>0.922414</td>\n",
       "      <td>0.929275</td>\n",
       "      <td>0.921296</td>\n",
       "      <td>0.923457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.293033</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.905869</td>\n",
       "      <td>0.895370</td>\n",
       "      <td>0.898329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.253602</td>\n",
       "      <td>0.922414</td>\n",
       "      <td>0.926590</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.923711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.257608</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.934773</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.931502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.4932    0.9000    0.6372        40\n",
      "     Neutral     0.8276    0.6667    0.7385        36\n",
      "    Positive     0.9286    0.3250    0.4815        40\n",
      "\n",
      "    accuracy                         0.6293       116\n",
      "   macro avg     0.7498    0.6306    0.6190       116\n",
      "weighted avg     0.7471    0.6293    0.6149       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-22\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-22/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-22/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-22/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-22/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-10] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7407    1.0000    0.8511        40\n",
      "     Neutral     1.0000    0.7778    0.8750        36\n",
      "    Positive     1.0000    0.8500    0.9189        40\n",
      "\n",
      "    accuracy                         0.8793       116\n",
      "   macro avg     0.9136    0.8759    0.8817       116\n",
      "weighted avg     0.9106    0.8793    0.8819       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-44\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-44/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-44/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-44/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-44/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-22] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8421    0.8000    0.8205        40\n",
      "     Neutral     1.0000    0.7778    0.8750        36\n",
      "    Positive     0.7600    0.9500    0.8444        40\n",
      "\n",
      "    accuracy                         0.8448       116\n",
      "   macro avg     0.8674    0.8426    0.8467       116\n",
      "weighted avg     0.8628    0.8448    0.8457       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-66\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-66/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-66/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-66/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-66/special_tokens_map.json\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8372    0.9000    0.8675        40\n",
      "     Neutral     1.0000    0.8056    0.8923        36\n",
      "    Positive     0.8636    0.9500    0.9048        40\n",
      "\n",
      "    accuracy                         0.8879       116\n",
      "   macro avg     0.9003    0.8852    0.8882       116\n",
      "weighted avg     0.8968    0.8879    0.8880       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-88\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-88/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-88/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-88/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-88/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-44] due to args.save_total_limit\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-66] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9444    0.8500    0.8947        40\n",
      "     Neutral     1.0000    0.8611    0.9254        36\n",
      "    Positive     0.8163    1.0000    0.8989        40\n",
      "\n",
      "    accuracy                         0.9052       116\n",
      "   macro avg     0.9203    0.9037    0.9063       116\n",
      "weighted avg     0.9175    0.9052    0.9057       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-110\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-110/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-110/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-110/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-110/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-88] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8444    0.9500    0.8941        40\n",
      "     Neutral     0.9697    0.8889    0.9275        36\n",
      "    Positive     0.9737    0.9250    0.9487        40\n",
      "\n",
      "    accuracy                         0.9224       116\n",
      "   macro avg     0.9293    0.9213    0.9235       116\n",
      "weighted avg     0.9279    0.9224    0.9233       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-132\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-132/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-132/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-132/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-132/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-110] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8571    0.9000    0.8780        40\n",
      "     Neutral     1.0000    0.8611    0.9254        36\n",
      "    Positive     0.8605    0.9250    0.8916        40\n",
      "\n",
      "    accuracy                         0.8966       116\n",
      "   macro avg     0.9059    0.8954    0.8983       116\n",
      "weighted avg     0.9026    0.8966    0.8974       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-154\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-154/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-154/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-154/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-154/special_tokens_map.json\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8605    0.9250    0.8916        40\n",
      "     Neutral     0.9706    0.9167    0.9429        36\n",
      "    Positive     0.9487    0.9250    0.9367        40\n",
      "\n",
      "    accuracy                         0.9224       116\n",
      "   macro avg     0.9266    0.9222    0.9237       116\n",
      "weighted avg     0.9251    0.9224    0.9231       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-176\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-176/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-176/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-132] due to args.save_total_limit\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-154] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9474    0.9000    0.9231        40\n",
      "     Neutral     0.9706    0.9167    0.9429        36\n",
      "    Positive     0.8864    0.9750    0.9286        40\n",
      "\n",
      "    accuracy                         0.9310       116\n",
      "   macro avg     0.9348    0.9306    0.9315       116\n",
      "weighted avg     0.9335    0.9310    0.9311       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-198\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-198/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-198/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-198/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-198/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-176] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from IJE_SA_MODEL/ije-sa-ft-indobert/checkpoint-198 (score: 0.9315018315018314).\n",
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobert\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobert/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobert/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobert/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobert/special_tokens_map.json\n",
      "loading configuration file IJE_SA_MODEL/ije-sa-ft-indobert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"IJE_SA_MODEL/ije-sa-ft-indobert\",\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {},\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file IJE_SA_MODEL/ije-sa-ft-indobert/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at IJE_SA_MODEL/ije-sa-ft-indobert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376c2e9158b946fabd6ecfe75864144d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91358ec5b891493ebd66f16aee3ed452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1345b8026e4104ab2204fedcbcc9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 463\n",
      "  Batch size = 32\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8609    0.9091    0.8844       143\n",
      "     Neutral     0.9671    0.9018    0.9333       163\n",
      "    Positive     0.9187    0.9363    0.9274       157\n",
      "\n",
      "    accuracy                         0.9158       463\n",
      "   macro avg     0.9156    0.9157    0.9150       463\n",
      "weighted avg     0.9179    0.9158    0.9162       463\n",
      "\n",
      "PredictionOutput(predictions=array([[ 4.528869  , -2.0435696 , -2.4441047 ],\n",
      "       [ 4.529774  , -2.8338451 , -1.7457023 ],\n",
      "       [ 4.354292  , -2.575475  , -1.7396991 ],\n",
      "       ...,\n",
      "       [-2.5459285 ,  3.4859998 , -0.0123857 ],\n",
      "       [ 3.9293919 , -3.2360063 , -0.8946097 ],\n",
      "       [-1.4233307 , -0.19207735,  1.9057432 ]], dtype=float32), label_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
      "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0,\n",
      "       0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0,\n",
      "       2]), metrics={'test_loss': 0.3542599081993103, 'test_accuracy': 0.9157667386609071, 'test_precision': 0.9155941384919251, 'test_recall': 0.9157457107908439, 'test_f1': 0.9150439565942045, 'test_runtime': 2.4884, 'test_samples_per_second': 186.066, 'test_steps_per_second': 6.028})\n",
      "Prediction finished!\n",
      "Total time: 2.2282021403312684 minutes\n"
     ]
    }
   ],
   "source": [
    "#5 indolem/indobert-base-uncased\n",
    "\n",
    "main(\n",
    "    model_name = 'indolem/indobert-base-uncased',\n",
    "    output_dir = 'IJE_SA_MODEL/ije-sa-ft-indobert',\n",
    "    best_params = 'best_parameters/indobert.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76615f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 9, 'learning_rate': 3e-05, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'weight_decay': 0.0002994715145894433}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobertweet-base-uncased/snapshots/32e28c05b47e33b6675d2670a1745c50a65e987a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"indolem/indobertweet-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobertweet-base-uncased/snapshots/32e28c05b47e33b6675d2670a1745c50a65e987a/pytorch_model.bin\n",
      "Some weights of the model checkpoint at indolem/indobertweet-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indolem/indobertweet-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobertweet-base-uncased/snapshots/32e28c05b47e33b6675d2670a1745c50a65e987a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"indolem/indobertweet-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobertweet-base-uncased/snapshots/32e28c05b47e33b6675d2670a1745c50a65e987a/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobertweet-base-uncased/snapshots/32e28c05b47e33b6675d2670a1745c50a65e987a/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobertweet-base-uncased/snapshots/32e28c05b47e33b6675d2670a1745c50a65e987a/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobertweet-base-uncased/snapshots/32e28c05b47e33b6675d2670a1745c50a65e987a/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobertweet-base-uncased/snapshots/32e28c05b47e33b6675d2670a1745c50a65e987a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"indolem/indobertweet-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--indolem--indobertweet-base-uncased/snapshots/32e28c05b47e33b6675d2670a1745c50a65e987a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"indolem/indobertweet-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57ce9ce9afc4026ad12e74a95d5f01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8f1dce2e3f4d8b894b20226d31e82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a714b4d0954324895a333e31002e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1350\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 198\n",
      "  Number of trainable parameters = 110560515\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/198 02:06, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.823800</td>\n",
       "      <td>0.504234</td>\n",
       "      <td>0.801724</td>\n",
       "      <td>0.822339</td>\n",
       "      <td>0.799074</td>\n",
       "      <td>0.803027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.331400</td>\n",
       "      <td>0.194686</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.934473</td>\n",
       "      <td>0.929630</td>\n",
       "      <td>0.931066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.125700</td>\n",
       "      <td>0.190074</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.933450</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.931562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.233398</td>\n",
       "      <td>0.913793</td>\n",
       "      <td>0.922025</td>\n",
       "      <td>0.912037</td>\n",
       "      <td>0.914572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>0.244056</td>\n",
       "      <td>0.922414</td>\n",
       "      <td>0.929545</td>\n",
       "      <td>0.921296</td>\n",
       "      <td>0.923646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.255440</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.929630</td>\n",
       "      <td>0.931844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.238821</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.941416</td>\n",
       "      <td>0.938889</td>\n",
       "      <td>0.939800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.260122</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.936850</td>\n",
       "      <td>0.929630</td>\n",
       "      <td>0.931704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.270884</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.936850</td>\n",
       "      <td>0.929630</td>\n",
       "      <td>0.931704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7381    0.7750    0.7561        40\n",
      "     Neutral     0.9630    0.7222    0.8254        36\n",
      "    Positive     0.7660    0.9000    0.8276        40\n",
      "\n",
      "    accuracy                         0.8017       116\n",
      "   macro avg     0.8223    0.7991    0.8030       116\n",
      "weighted avg     0.8175    0.8017    0.8023       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-22\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-22/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-22/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-22/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-22/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-7] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8837    0.9500    0.9157        40\n",
      "     Neutral     0.9697    0.8889    0.9275        36\n",
      "    Positive     0.9500    0.9500    0.9500        40\n",
      "\n",
      "    accuracy                         0.9310       116\n",
      "   macro avg     0.9345    0.9296    0.9311       116\n",
      "weighted avg     0.9333    0.9310    0.9312       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-44\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-44/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-44/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-44/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-44/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-22] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9048    0.9500    0.9268        40\n",
      "     Neutral     0.9706    0.9167    0.9429        36\n",
      "    Positive     0.9250    0.9250    0.9250        40\n",
      "\n",
      "    accuracy                         0.9310       116\n",
      "   macro avg     0.9335    0.9306    0.9316       116\n",
      "weighted avg     0.9322    0.9310    0.9312       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-66\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-66/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-66/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-66/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-66/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-44] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9024    0.9250    0.9136        40\n",
      "     Neutral     1.0000    0.8611    0.9254        36\n",
      "    Positive     0.8636    0.9500    0.9048        40\n",
      "\n",
      "    accuracy                         0.9138       116\n",
      "   macro avg     0.9220    0.9120    0.9146       116\n",
      "weighted avg     0.9193    0.9138    0.9142       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-88\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-88/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-88/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-88/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-88/special_tokens_map.json\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9250    0.9250    0.9250        40\n",
      "     Neutral     1.0000    0.8889    0.9412        36\n",
      "    Positive     0.8636    0.9500    0.9048        40\n",
      "\n",
      "    accuracy                         0.9224       116\n",
      "   macro avg     0.9295    0.9213    0.9236       116\n",
      "weighted avg     0.9271    0.9224    0.9230       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-110\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-110/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-110/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-110/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-110/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-88] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9487    0.9250    0.9367        40\n",
      "     Neutral     1.0000    0.8889    0.9412        36\n",
      "    Positive     0.8667    0.9750    0.9176        40\n",
      "\n",
      "    accuracy                         0.9310       116\n",
      "   macro avg     0.9385    0.9296    0.9318       116\n",
      "weighted avg     0.9363    0.9310    0.9315       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-132\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-132/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-132/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-132/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-132/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-66] due to args.save_total_limit\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-110] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9268    0.9500    0.9383        40\n",
      "     Neutral     0.9706    0.9167    0.9429        36\n",
      "    Positive     0.9268    0.9500    0.9383        40\n",
      "\n",
      "    accuracy                         0.9397       116\n",
      "   macro avg     0.9414    0.9389    0.9398       116\n",
      "weighted avg     0.9404    0.9397    0.9397       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-154\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-154/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-154/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-154/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-154/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-132] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9268    0.9500    0.9383        40\n",
      "     Neutral     1.0000    0.8889    0.9412        36\n",
      "    Positive     0.8837    0.9500    0.9157        40\n",
      "\n",
      "    accuracy                         0.9310       116\n",
      "   macro avg     0.9369    0.9296    0.9317       116\n",
      "weighted avg     0.9347    0.9310    0.9314       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-176\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-176/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-176/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-176/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-176/special_tokens_map.json\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9268    0.9500    0.9383        40\n",
      "     Neutral     1.0000    0.8889    0.9412        36\n",
      "    Positive     0.8837    0.9500    0.9157        40\n",
      "\n",
      "    accuracy                         0.9310       116\n",
      "   macro avg     0.9369    0.9296    0.9317       116\n",
      "weighted avg     0.9347    0.9310    0.9314       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-198\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-198/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-198/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-198/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-198/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-176] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-154 (score: 0.9398001175778953).\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-indobertweet/checkpoint-198] due to args.save_total_limit\n",
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-indobertweet\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-indobertweet/special_tokens_map.json\n",
      "loading configuration file IJE_SA_MODEL/ije-sa-ft-indobertweet/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"IJE_SA_MODEL/ije-sa-ft-indobertweet\",\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {},\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31923\n",
      "}\n",
      "\n",
      "loading weights file IJE_SA_MODEL/ije-sa-ft-indobertweet/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at IJE_SA_MODEL/ije-sa-ft-indobertweet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bd9cddda6b4b23919290bcf64b493f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76844f2fc4a4e139583f72802613176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a6192c6ba540e0bd0dd967128427d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 463\n",
      "  Batch size = 32\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8903    0.9650    0.9262       143\n",
      "     Neutral     0.9934    0.9202    0.9554       163\n",
      "    Positive     0.9427    0.9427    0.9427       157\n",
      "\n",
      "    accuracy                         0.9417       463\n",
      "   macro avg     0.9421    0.9427    0.9414       463\n",
      "weighted avg     0.9444    0.9417    0.9421       463\n",
      "\n",
      "PredictionOutput(predictions=array([[ 4.273509  , -2.5944867 , -2.377041  ],\n",
      "       [ 4.820379  , -3.3244283 , -2.5188272 ],\n",
      "       [ 4.5697417 , -3.130979  , -2.3377383 ],\n",
      "       ...,\n",
      "       [-2.565445  ,  1.1667743 ,  2.3371422 ],\n",
      "       [ 4.6501203 , -3.131346  , -2.320866  ],\n",
      "       [-2.52371   ,  0.02041547,  3.0547304 ]], dtype=float32), label_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
      "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0,\n",
      "       0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0,\n",
      "       2]), metrics={'test_loss': 0.22201459109783173, 'test_accuracy': 0.9416846652267818, 'test_precision': 0.9421250744415129, 'test_recall': 0.9426518410145466, 'test_f1': 0.9414212228729392, 'test_runtime': 2.3888, 'test_samples_per_second': 193.821, 'test_steps_per_second': 6.279})\n",
      "Prediction finished!\n",
      "Total time: 2.272595461209615 minutes\n"
     ]
    }
   ],
   "source": [
    "#6 indolem/indobertweet-base-uncased\n",
    "\n",
    "main(\n",
    "    model_name = 'indolem/indobertweet-base-uncased',\n",
    "    output_dir = 'IJE_SA_MODEL/ije-sa-ft-indobertweet',\n",
    "    best_params = 'best_parameters/indobertweet.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be0f2b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 5, 'learning_rate': 0.0001, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 32, 'weight_decay': 9.512565513083272e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--bert-base-multilingual-uncased/snapshots/3da6b6aad5111664db74322f2158b7f93e09a717/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435976c6d6d44146937355b53389d0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d6d190097d461b871b25be96ebd5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a72875b5dc64777b2c62b86905ecdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1350\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 215\n",
      "  Number of trainable parameters = 167358723\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='215' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [215/215 02:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.879300</td>\n",
       "      <td>0.597996</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.743216</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.656083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.540637</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.863814</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.794734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.400315</td>\n",
       "      <td>0.887931</td>\n",
       "      <td>0.896537</td>\n",
       "      <td>0.885185</td>\n",
       "      <td>0.887587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.371477</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.884650</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>0.881923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.379420</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.884650</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>0.881923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.5286    0.9250    0.6727        40\n",
      "     Neutral     0.9677    0.8333    0.8955        36\n",
      "    Positive     0.7333    0.2750    0.4000        40\n",
      "\n",
      "    accuracy                         0.6724       116\n",
      "   macro avg     0.7432    0.6778    0.6561       116\n",
      "weighted avg     0.7355    0.6724    0.6478       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-43\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-43/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-43/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-43/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-43/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-16] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9565    0.5500    0.6984        40\n",
      "     Neutral     1.0000    0.8333    0.9091        36\n",
      "    Positive     0.6349    1.0000    0.7767        40\n",
      "\n",
      "    accuracy                         0.7931       116\n",
      "   macro avg     0.8638    0.7944    0.7947       116\n",
      "weighted avg     0.8591    0.7931    0.7908       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-86\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-86/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-86/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-86/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-86/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-43] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8182    0.9000    0.8571        40\n",
      "     Neutral     0.9667    0.8056    0.8788        36\n",
      "    Positive     0.9048    0.9500    0.9268        40\n",
      "\n",
      "    accuracy                         0.8879       116\n",
      "   macro avg     0.8965    0.8852    0.8876       116\n",
      "weighted avg     0.8941    0.8879    0.8879       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-129\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-129/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-129/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-129/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-129/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-86] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8462    0.8250    0.8354        40\n",
      "     Neutral     0.9706    0.9167    0.9429        36\n",
      "    Positive     0.8372    0.9000    0.8675        40\n",
      "\n",
      "    accuracy                         0.8793       116\n",
      "   macro avg     0.8847    0.8806    0.8819       116\n",
      "weighted avg     0.8817    0.8793    0.8798       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-172\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-172/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-172/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-172/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-172/special_tokens_map.json\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8462    0.8250    0.8354        40\n",
      "     Neutral     0.9706    0.9167    0.9429        36\n",
      "    Positive     0.8372    0.9000    0.8675        40\n",
      "\n",
      "    accuracy                         0.8793       116\n",
      "   macro avg     0.8847    0.8806    0.8819       116\n",
      "weighted avg     0.8817    0.8793    0.8798       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-215\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-215/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-215/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-215/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-215/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-172] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-129 (score: 0.887586668074473).\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-mbert/checkpoint-215] due to args.save_total_limit\n",
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-mbert\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-mbert/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-mbert/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-mbert/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-mbert/special_tokens_map.json\n",
      "loading configuration file IJE_SA_MODEL/ije-sa-ft-mbert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"IJE_SA_MODEL/ije-sa-ft-mbert\",\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {},\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading weights file IJE_SA_MODEL/ije-sa-ft-mbert/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at IJE_SA_MODEL/ije-sa-ft-mbert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d673dd40dc743bb892805de9e7e34c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb18fad0c5946e9ae4e713a11fd9c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ff9d3ab83746e2b28265d6414f99b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 463\n",
      "  Batch size = 32\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.6888    0.9441    0.7965       143\n",
      "     Neutral     0.9846    0.7853    0.8737       163\n",
      "    Positive     0.8978    0.7834    0.8367       157\n",
      "\n",
      "    accuracy                         0.8337       463\n",
      "   macro avg     0.8571    0.8376    0.8356       463\n",
      "weighted avg     0.8638    0.8337    0.8373       463\n",
      "\n",
      "PredictionOutput(predictions=array([[ 2.9048173 , -2.5359194 , -0.8562975 ],\n",
      "       [ 2.8924458 , -2.4742615 , -0.9189515 ],\n",
      "       [ 2.8946009 , -2.4517972 , -0.9423044 ],\n",
      "       ...,\n",
      "       [ 0.05830768, -2.601041  ,  2.420907  ],\n",
      "       [ 2.9236724 , -2.5925229 , -0.7496682 ],\n",
      "       [ 0.6142814 , -2.726361  ,  2.0233245 ]], dtype=float32), label_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
      "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0,\n",
      "       0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0,\n",
      "       2]), metrics={'test_loss': 0.5963976979255676, 'test_accuracy': 0.8336933045356372, 'test_precision': 0.8570670379325228, 'test_recall': 0.8375905027071453, 'test_f1': 0.8356383357958244, 'test_runtime': 2.9149, 'test_samples_per_second': 158.84, 'test_steps_per_second': 5.146})\n",
      "Prediction finished!\n",
      "Total time: 2.311599604288737 minutes\n"
     ]
    }
   ],
   "source": [
    "#7 bert-base-multilingual-uncased\n",
    "\n",
    "main(\n",
    "    model_name = 'bert-base-multilingual-uncased',\n",
    "    output_dir = 'IJE_SA_MODEL/ije-sa-ft-mbert',\n",
    "    best_params = 'best_parameters/mbert.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "902b304b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 6, 'learning_rate': 0.0001, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 32, 'weight_decay': 0.0012506415538934051}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/9e90d6dd84b6a2e4d65e4d751baed6cd56578fd3/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-multilingual-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/9e90d6dd84b6a2e4d65e4d751baed6cd56578fd3/model.safetensors\n",
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/9e90d6dd84b6a2e4d65e4d751baed6cd56578fd3/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-multilingual-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/9e90d6dd84b6a2e4d65e4d751baed6cd56578fd3/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/9e90d6dd84b6a2e4d65e4d751baed6cd56578fd3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/9e90d6dd84b6a2e4d65e4d751baed6cd56578fd3/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--distilbert-base-multilingual-cased/snapshots/9e90d6dd84b6a2e4d65e4d751baed6cd56578fd3/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-multilingual-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7c1fb42bd4497a8c807ed5e111e746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e31602c16c497a80650bcc786a96f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe4bd70b348456685459e25cbde160d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1350\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 132\n",
      "  Number of trainable parameters = 135326979\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 01:14, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.883800</td>\n",
       "      <td>0.737343</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.673898</td>\n",
       "      <td>0.657407</td>\n",
       "      <td>0.658746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.545500</td>\n",
       "      <td>0.495415</td>\n",
       "      <td>0.853448</td>\n",
       "      <td>0.879023</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.854445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.261300</td>\n",
       "      <td>0.456711</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.849294</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.846113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.479437</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.871044</td>\n",
       "      <td>0.845370</td>\n",
       "      <td>0.845339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.367972</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.885587</td>\n",
       "      <td>0.880556</td>\n",
       "      <td>0.879222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.346611</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.899692</td>\n",
       "      <td>0.896296</td>\n",
       "      <td>0.897309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.5769    0.7500    0.6522        40\n",
      "     Neutral     0.8387    0.7222    0.7761        36\n",
      "    Positive     0.6061    0.5000    0.5479        40\n",
      "\n",
      "    accuracy                         0.6552       116\n",
      "   macro avg     0.6739    0.6574    0.6587       116\n",
      "weighted avg     0.6682    0.6552    0.6547       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-22\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-22/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-22/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-22/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-22/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-40] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8824    0.7500    0.8108        40\n",
      "     Neutral     1.0000    0.8056    0.8923        36\n",
      "    Positive     0.7547    1.0000    0.8602        40\n",
      "\n",
      "    accuracy                         0.8534       116\n",
      "   macro avg     0.8790    0.8519    0.8544       116\n",
      "weighted avg     0.8749    0.8534    0.8531       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-44\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-44/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-44/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-44/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-44/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-22] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8095    0.8500    0.8293        40\n",
      "     Neutral     0.9091    0.8333    0.8696        36\n",
      "    Positive     0.8293    0.8500    0.8395        40\n",
      "\n",
      "    accuracy                         0.8448       116\n",
      "   macro avg     0.8493    0.8444    0.8461       116\n",
      "weighted avg     0.8472    0.8448    0.8453       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-66\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-66/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-66/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-66/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-66/special_tokens_map.json\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7358    0.9750    0.8387        40\n",
      "     Neutral     0.9118    0.8611    0.8857        36\n",
      "    Positive     0.9655    0.7000    0.8116        40\n",
      "\n",
      "    accuracy                         0.8448       116\n",
      "   macro avg     0.8710    0.8454    0.8453       116\n",
      "weighted avg     0.8696    0.8448    0.8439       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-88\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-88/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-88/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-88/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-88/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-66] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9697    0.8000    0.8767        40\n",
      "     Neutral     0.8462    0.9167    0.8800        36\n",
      "    Positive     0.8409    0.9250    0.8810        40\n",
      "\n",
      "    accuracy                         0.8793       116\n",
      "   macro avg     0.8856    0.8806    0.8792       116\n",
      "weighted avg     0.8869    0.8793    0.8792       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-110\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-110/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-110/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-110/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-110/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-44] due to args.save_total_limit\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-88] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8974    0.8750    0.8861        40\n",
      "     Neutral     0.9412    0.8889    0.9143        36\n",
      "    Positive     0.8605    0.9250    0.8916        40\n",
      "\n",
      "    accuracy                         0.8966       116\n",
      "   macro avg     0.8997    0.8963    0.8973       116\n",
      "weighted avg     0.8983    0.8966    0.8967       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-132\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-132/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-132/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-132/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-132/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-110] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from IJE_SA_MODEL/ije-sa-ft-distilmbert/checkpoint-132 (score: 0.8973093095710146).\n",
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-distilmbert\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-distilmbert/special_tokens_map.json\n",
      "loading configuration file IJE_SA_MODEL/ije-sa-ft-distilmbert/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"IJE_SA_MODEL/ije-sa-ft-distilmbert\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {},\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file IJE_SA_MODEL/ije-sa-ft-distilmbert/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at IJE_SA_MODEL/ije-sa-ft-distilmbert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c872cb546cc405ba90f036a29ca87fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9da2a8cff5741aa934ceeb1d5956eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b4d1c42c4e49b5bbe32a344302643b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 463\n",
      "  Batch size = 32\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.7095    0.8881    0.7888       143\n",
      "     Neutral     0.9236    0.8160    0.8664       163\n",
      "    Positive     0.8643    0.7707    0.8148       157\n",
      "\n",
      "    accuracy                         0.8229       463\n",
      "   macro avg     0.8325    0.8249    0.8234       463\n",
      "weighted avg     0.8374    0.8229    0.8250       463\n",
      "\n",
      "PredictionOutput(predictions=array([[ 3.3847132 , -2.7222726 , -1.4551932 ],\n",
      "       [ 3.699325  , -2.6873791 , -1.8124403 ],\n",
      "       [ 3.662219  , -2.780971  , -1.6497129 ],\n",
      "       ...,\n",
      "       [ 2.317116  , -1.0851647 , -1.5622102 ],\n",
      "       [ 2.8477736 , -3.3111115 , -0.42939085],\n",
      "       [-1.8452371 , -1.3140638 ,  3.24251   ]], dtype=float32), label_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
      "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0,\n",
      "       0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0,\n",
      "       2]), metrics={'test_loss': 0.6505388021469116, 'test_accuracy': 0.8228941684665226, 'test_precision': 0.8324646773669121, 'test_recall': 0.8249211484333207, 'test_f1': 0.8233614006639547, 'test_runtime': 1.7837, 'test_samples_per_second': 259.573, 'test_steps_per_second': 8.409})\n",
      "Prediction finished!\n",
      "Total time: 1.4191406806310018 minutes\n"
     ]
    }
   ],
   "source": [
    "#8 distilbert-base-multilingual-cased\n",
    "\n",
    "main(\n",
    "    model_name = 'distilbert-base-multilingual-cased',\n",
    "    output_dir = 'IJE_SA_MODEL/ije-sa-ft-distilmbert',\n",
    "    best_params = 'best_parameters/distilmbert.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd1c8ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epochs': 9, 'learning_rate': 0.0001, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 64, 'weight_decay': 0.0011724870810857187}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/77de1f7a7e5e737aead1cd880979d4f1b3af6668/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/77de1f7a7e5e737aead1cd880979d4f1b3af6668/model.safetensors\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/77de1f7a7e5e737aead1cd880979d4f1b3af6668/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/77de1f7a7e5e737aead1cd880979d4f1b3af6668/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/77de1f7a7e5e737aead1cd880979d4f1b3af6668/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /home/jupyter-145230403/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/77de1f7a7e5e737aead1cd880979d4f1b3af6668/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2068b672b6f74f95b7f84582dabce42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8c1c9098b64a658c2c82003d5b4e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdecfa901cd6436ea9ae8cacd71ae197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1350\n",
      "  Num Epochs = 9\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 54\n",
      "  Number of trainable parameters = 278045955\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 02:43, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.069300</td>\n",
       "      <td>0.884477</td>\n",
       "      <td>0.732759</td>\n",
       "      <td>0.794156</td>\n",
       "      <td>0.726852</td>\n",
       "      <td>0.730395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.446969</td>\n",
       "      <td>0.887931</td>\n",
       "      <td>0.891709</td>\n",
       "      <td>0.887037</td>\n",
       "      <td>0.887650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.277743</td>\n",
       "      <td>0.922414</td>\n",
       "      <td>0.924621</td>\n",
       "      <td>0.919444</td>\n",
       "      <td>0.920308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.276700</td>\n",
       "      <td>0.373452</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.880589</td>\n",
       "      <td>0.858333</td>\n",
       "      <td>0.861004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.303768</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>0.907105</td>\n",
       "      <td>0.906481</td>\n",
       "      <td>0.904861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.164700</td>\n",
       "      <td>0.291812</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>0.912879</td>\n",
       "      <td>0.904630</td>\n",
       "      <td>0.906979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.301379</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.896930</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>0.897118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.364221</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.896885</td>\n",
       "      <td>0.876852</td>\n",
       "      <td>0.880169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.345618</td>\n",
       "      <td>0.879310</td>\n",
       "      <td>0.894504</td>\n",
       "      <td>0.876852</td>\n",
       "      <td>0.880290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8235    0.7000    0.7568        40\n",
      "     Neutral     0.9524    0.5556    0.7018        36\n",
      "    Positive     0.6066    0.9250    0.7327        40\n",
      "\n",
      "    accuracy                         0.7328       116\n",
      "   macro avg     0.7942    0.7269    0.7304       116\n",
      "weighted avg     0.7887    0.7328    0.7314       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-6\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-6/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-6/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-6/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-6/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-9] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9189    0.8500    0.8831        40\n",
      "     Neutral     0.9118    0.8611    0.8857        36\n",
      "    Positive     0.8444    0.9500    0.8941        40\n",
      "\n",
      "    accuracy                         0.8879       116\n",
      "   macro avg     0.8917    0.8870    0.8876       116\n",
      "weighted avg     0.8910    0.8879    0.8877       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-12\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-12/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-12/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-12/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-12/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-6] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8864    0.9750    0.9286        40\n",
      "     Neutral     0.9375    0.8333    0.8824        36\n",
      "    Positive     0.9500    0.9500    0.9500        40\n",
      "\n",
      "    accuracy                         0.9224       116\n",
      "   macro avg     0.9246    0.9194    0.9203       116\n",
      "weighted avg     0.9242    0.9224    0.9216       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-18\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-18/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-18/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-18/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-18/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-12] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8293    0.8500    0.8395        40\n",
      "     Neutral     1.0000    0.7500    0.8571        36\n",
      "    Positive     0.8125    0.9750    0.8864        40\n",
      "\n",
      "    accuracy                         0.8621       116\n",
      "   macro avg     0.8806    0.8583    0.8610       116\n",
      "weighted avg     0.8765    0.8621    0.8611       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-24\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-24/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-24/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-24/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-24/special_tokens_map.json\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9429    0.8250    0.8800        40\n",
      "     Neutral     0.8947    0.9444    0.9189        36\n",
      "    Positive     0.8837    0.9500    0.9157        40\n",
      "\n",
      "    accuracy                         0.9052       116\n",
      "   macro avg     0.9071    0.9065    0.9049       116\n",
      "weighted avg     0.9075    0.9052    0.9044       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-30\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-30/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-30/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-24] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8750    0.8750    0.8750        40\n",
      "     Neutral     1.0000    0.8889    0.9412        36\n",
      "    Positive     0.8636    0.9500    0.9048        40\n",
      "\n",
      "    accuracy                         0.9052       116\n",
      "   macro avg     0.9129    0.9046    0.9070       116\n",
      "weighted avg     0.9099    0.9052    0.9058       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-36\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-36/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-36/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-36/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-36/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-30] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8750    0.8750    0.8750        40\n",
      "     Neutral     0.8947    0.9444    0.9189        36\n",
      "    Positive     0.9211    0.8750    0.8974        40\n",
      "\n",
      "    accuracy                         0.8966       116\n",
      "   macro avg     0.8969    0.8981    0.8971       116\n",
      "weighted avg     0.8970    0.8966    0.8964       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-42\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-42/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-42/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-42/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-42/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-36] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8947    0.8500    0.8718        40\n",
      "     Neutral     1.0000    0.8056    0.8923        36\n",
      "    Positive     0.7959    0.9750    0.8764        40\n",
      "\n",
      "    accuracy                         0.8793       116\n",
      "   macro avg     0.8969    0.8769    0.8802       116\n",
      "weighted avg     0.8933    0.8793    0.8798       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-48\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-48/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-42] due to args.save_total_limit\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 116\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8750    0.8750    0.8750        40\n",
      "     Neutral     1.0000    0.8056    0.8923        36\n",
      "    Positive     0.8085    0.9500    0.8736        40\n",
      "\n",
      "    accuracy                         0.8793       116\n",
      "   macro avg     0.8945    0.8769    0.8803       116\n",
      "weighted avg     0.8909    0.8793    0.8799       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-54\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-54/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-54/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-54/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-54/special_tokens_map.json\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-48] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-18 (score: 0.9203081232492997).\n",
      "Deleting older checkpoint [IJE_SA_MODEL/ije-sa-ft-xlm-roberta/checkpoint-54] due to args.save_total_limit\n",
      "Saving model checkpoint to IJE_SA_MODEL/ije-sa-ft-xlm-roberta\n",
      "Configuration saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/config.json\n",
      "Model weights saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/pytorch_model.bin\n",
      "tokenizer config file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/tokenizer_config.json\n",
      "Special tokens file saved in IJE_SA_MODEL/ije-sa-ft-xlm-roberta/special_tokens_map.json\n",
      "loading configuration file IJE_SA_MODEL/ije-sa-ft-xlm-roberta/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"IJE_SA_MODEL/ije-sa-ft-xlm-roberta\",\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {},\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"Negative\",\n",
      "    \"1\": \"Neutral\",\n",
      "    \"2\": \"Positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"Negative\": 0,\n",
      "    \"Neutral\": 1,\n",
      "    \"Positive\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file IJE_SA_MODEL/ije-sa-ft-xlm-roberta/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at IJE_SA_MODEL/ije-sa-ft-xlm-roberta.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d12f50f51344148d88f4212a0ed7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d317c7d48964ff4848cb9619a0c2cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ce9e052aa64a6c996892c9775ee7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tweet, clean_tweet. If tweet, clean_tweet are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 463\n",
      "  Batch size = 32\n",
      "/home/jupyter-145230403/.conda/envs/conda_env_2/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.8012    0.9580    0.8726       143\n",
      "     Neutral     0.9452    0.8466    0.8932       163\n",
      "    Positive     0.9315    0.8662    0.8977       157\n",
      "\n",
      "    accuracy                         0.8877       463\n",
      "   macro avg     0.8926    0.8903    0.8878       463\n",
      "weighted avg     0.8961    0.8877    0.8884       463\n",
      "\n",
      "PredictionOutput(predictions=array([[ 1.439427  , -2.0475745 ,  0.32817125],\n",
      "       [ 2.6250486 , -2.1165354 , -0.85414433],\n",
      "       [ 2.3578537 , -1.1590322 , -1.4477365 ],\n",
      "       ...,\n",
      "       [-1.6875925 ,  2.3683002 , -1.2940696 ],\n",
      "       [ 2.693726  , -1.9362189 , -0.97334987],\n",
      "       [-1.3465405 , -0.7248404 ,  1.3096148 ]], dtype=float32), label_ids=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2,\n",
      "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0,\n",
      "       0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       2, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0,\n",
      "       2]), metrics={'test_loss': 0.3732810616493225, 'test_accuracy': 0.8876889848812095, 'test_precision': 0.8926273064701326, 'test_recall': 0.8903032543765613, 'test_f1': 0.887835039146732, 'test_runtime': 2.7019, 'test_samples_per_second': 171.363, 'test_steps_per_second': 5.552})\n",
      "Prediction finished!\n",
      "Total time: 3.105742116769155 minutes\n"
     ]
    }
   ],
   "source": [
    "#9 xlm-roberta-base\n",
    "\n",
    "main(\n",
    "    model_name = 'xlm-roberta-base',\n",
    "    output_dir = 'IJE_SA_MODEL/ije-sa-ft-xlm-roberta',\n",
    "    best_params = 'best_parameters/xlm-roberta.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f0832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env_2",
   "language": "python",
   "name": "conda_env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
